{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f387ae5-1bea-4704-a266-2c88adad54f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from PIL import Image, ImageOps\n",
    "import urllib\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MnistData:\n",
    "    image_size = 28*28\n",
    "    dataset_dir = 'dataset'\n",
    "    dataset_pkl = 'mnist.pkl'\n",
    "    url_base = 'https://jrkwon.com/data/ece5831/mnist/'\n",
    "\n",
    "    key_file = {\n",
    "        'train_images': 'train-images-idx3-ubyte.gz',\n",
    "        'train_labels': 'train-labels-idx1-ubyte.gz',\n",
    "        'test_images':  't10k-images-idx3-ubyte.gz',\n",
    "        'test_labels':  't10k-labels-idx1-ubyte.gz'\n",
    "    }\n",
    "\n",
    "    file_name = key_file['train_images']\n",
    "    file_path = url_base + file_name\n",
    "    \n",
    "    def __init__(self, image_size):\n",
    "        self.image_size = image_size\n",
    "        self.url_base = 'https://jrkwon.com/data/ece5831/mnist/'\n",
    "\n",
    "    def softmax(self, a):\n",
    "        return np.exp(a) / np.sum(np.exp(a))\n",
    "\n",
    "    def softmax_modified(self, a):\n",
    "        c = np.max(a)\n",
    "        exp_a = np.exp(a - c)\n",
    "        return exp_a / np.sum(exp_a)\n",
    "\n",
    "    def _load_images(self, file_name):\n",
    "        with gzip.open(file_name, 'rb') as f:\n",
    "            images = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "        images = images.reshape(-1, self.image_size)\n",
    "        return images\n",
    "\n",
    "    def _load_labels(self, file_name):\n",
    "        with gzip.open(file_name, 'rb') as f:\n",
    "            labels = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "        return labels\n",
    "\n",
    "    def download(self, file_name):\n",
    "        file_path = os.path.join(dataset_dir, file_name)\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            print(f'File: {file_name} already exists')\n",
    "            return\n",
    "\n",
    "        print(f'Downloading {file_name}...')\n",
    "        urllib.request.urlretrieve(self.url_base + file_name, file_path)\n",
    "\n",
    "    def _download_all(self):\n",
    "        for file_name in key_file.values():\n",
    "            self.download(file_name)\n",
    "\n",
    "    def change_one_hot_label(self, y, num_class, idx):\n",
    "        t = np.zeros((y.size, num_class))\n",
    "        for idx, row in enumerate(t):\n",
    "            row[y[idx]] = 1\n",
    "        return t\n",
    "\n",
    "    def _create_dataset(self, key_file, dataset_dir, dataset_pkl, file_name1, file_name2, file_name3, file_name4):\n",
    "        dataset = {}\n",
    "        dataset[file_name1] = self._load_images(os.path.join(dataset_dir, key_file[file_name1]))\n",
    "        dataset[file_name2] = self._load_labels(os.path.join(dataset_dir, key_file[file_name2]))\n",
    "        dataset[file_name3] = self._load_images(os.path.join(dataset_dir, key_file[file_name3]))\n",
    "        dataset[file_name4] = self._load_labels(os.path.join(dataset_dir, key_file[file_name4]))\n",
    "\n",
    "        with open(os.path.join(dataset_dir, dataset_pkl), 'wb') as f:  # Change to 'wb'\n",
    "            print(f'Pickle: {dataset_dir}/{dataset_pkl} is being created.')\n",
    "            pickle.dump(dataset, f)\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    def init_dataset(self, key_file, dataset_dir, dataset_pkl, file_name1, file_name2, file_name3, file_name4):\n",
    "        self._download_all()\n",
    "        if os.path.exists(os.path.join(dataset_dir, dataset_pkl)):\n",
    "            with open(os.path.join(dataset_dir, dataset_pkl), 'rb') as f:\n",
    "                dataset = pickle.load(f) \n",
    "        else:\n",
    "            dataset = self._create_dataset(key_file, dataset_dir, dataset_pkl, file_name1, file_name2, file_name3, file_name4)\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    def load_image(file_path):\n",
    "        # Replace this with the path to your image\n",
    "        image = Image.open(file_path).convert(\"RGB\")\n",
    "        return image\n",
    "\n",
    "    def init():\n",
    "        # Disable scientific notation for clarity\n",
    "        np.set_printoptions(suppress=True)\n",
    "\n",
    "    def load_my_model():\n",
    "        # Load the model\n",
    "        model = load_model(\"model/keras_model.h5\")\n",
    "\n",
    "        # Load the labels\n",
    "        class_names = open(\"model/labels.txt\", \"r\").readlines()\n",
    "\n",
    "        return model, class_names\n",
    "\n",
    "    def prep_input(image):\n",
    "        # Create the array of the right shape to feed into the keras model\n",
    "        data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32)\n",
    "\n",
    "        # Resize the image to be 224x224 and crop from the center\n",
    "        size = (224, 224)\n",
    "        image = ImageOps.fit(image, size, Image.Resampling.LANCZOS)\n",
    "\n",
    "        # Convert image to numpy array and normalize\n",
    "        image_array = np.asarray(image)\n",
    "        normalized_image_array = (image_array.astype(np.float32) / 127.5) - 1\n",
    "        data[0] = normalized_image_array\n",
    "\n",
    "        return data\n",
    "\n",
    "    def predict(model, class_names, data):\n",
    "        # Make a prediction\n",
    "        prediction = model.predict(data)\n",
    "        index = np.argmax(prediction)\n",
    "        class_name = class_names[index]\n",
    "        confidence_score = prediction[0][index]\n",
    "\n",
    "        # Print the prediction and confidence score\n",
    "        print(\"Class:\", class_name[2:], end=\"\")\n",
    "        print(\"Confidence Score:\", confidence_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1346c41c-8395-4664-9357-b2da32d181a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Initialize the MnistData class\n",
    "    mnist_data = MnistData(image_size=28*28)\n",
    "\n",
    "    # Test downloading the dataset\n",
    "    print(\"Testing dataset download:\")\n",
    "    mnist_data._download_all()\n",
    "\n",
    "    # Check if the dataset files exist\n",
    "    for file_name in mnist_data.key_file.values():\n",
    "        file_path = os.path.join(mnist_data.dataset_dir, file_name)\n",
    "        assert os.path.exists(file_path), f\"Dataset file {file_name} does not exist.\"\n",
    "    print(\"All dataset files downloaded successfully.\\n\")\n",
    "\n",
    "    # Test loading images and labels\n",
    "    print(\"Testing loading images and labels:\")\n",
    "    train_images = mnist_data._load_images(mnist_data.key_file['train_images'])\n",
    "    train_labels = mnist_data._load_labels(mnist_data.key_file['train_labels'])\n",
    "    \n",
    "    assert train_images.shape == (60000, mnist_data.image_size), f\"Expected shape (60000, {mnist_data.image_size}), got {train_images.shape}\"\n",
    "    assert train_labels.shape == (60000,), f\"Expected shape (60000,), got {train_labels.shape}\"\n",
    "    print(\"Images and labels loaded successfully.\\n\")\n",
    "\n",
    "    # Test softmax function\n",
    "    print(\"Testing softmax function:\")\n",
    "    test_array = np.array([1.0, 2.0, 3.0])\n",
    "    softmax_output = mnist_data.softmax(test_array)\n",
    "    print(\"Softmax output:\", softmax_output)\n",
    "    assert np.allclose(np.sum(softmax_output), 1), \"Softmax output does not sum to 1.\"\n",
    "    print(\"Softmax function works correctly.\\n\")\n",
    "\n",
    "    # Test change_one_hot_label function\n",
    "    print(\"Testing change_one_hot_label function:\")\n",
    "    labels = np.array([0, 1, 2])\n",
    "    one_hot_labels = mnist_data.change_one_hot_label(labels, num_class=3, idx=0)\n",
    "    print(\"One-hot labels:\", one_hot_labels)\n",
    "    assert one_hot_labels.shape == (3, 3), f\"Expected shape (3, 3), got {one_hot_labels.shape}\"\n",
    "    assert np.array_equal(one_hot_labels[0], [1, 0, 0]), \"First label is not one-hot encoded correctly.\"\n",
    "    print(\"One-hot label conversion works correctly.\\n\")\n",
    "\n",
    "    # Test model loading and prediction functions\n",
    "    print(\"Testing model loading and prediction functions:\")\n",
    "    try:\n",
    "        model, class_names = mnist_data.load_my_model()\n",
    "        print(\"Model and class names loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error loading model or class names:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497aeea-abf3-426e-aa1e-3e5a44965701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
